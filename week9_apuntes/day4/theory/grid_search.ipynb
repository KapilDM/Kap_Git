{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "cf88cdeb52cfbd9b7891bd0104565f5c75083759e3ad4e400c6eab5e28be6642"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is GridSearch?\n",
    "GridSearch is an optimization tool that we use when tuning hyperparameters. We define the grid of parameters that we want to search through, and we select the best combination of parameters for our data.\n",
    "\n",
    "https://towardsdatascience.com/gridsearch-the-ultimate-machine-learning-tool-6cd5fb93d07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBA DIFERENTES PARAMETROS PARA LOS ALGORITMOS Y EL OBJETIVO ES ENCONTRAR CUAL ES EL QUE DA MEJOR ACCURACY PARA LOS DATOS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The “Grid” in GridSearch\n",
    "\n",
    "![grid](grid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: One way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) #Esto lo pone porque hay algunas librerias que van a estar a punto de caducarse y esto es para que no salgan los warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "clf.best_stimator_ SVC(C=0.5, coef0=-10.0, degree=1, kernel='linear')\nclf.best_params_ {'C': 0.5, 'coef0': -10.0, 'degree': 1, 'gamma': 'scale', 'kernel': 'linear'}\nclf.best_score 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets #importamos datasets praa coger datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# 'kernel':('linear', 'poly', 'rbf', 'sigmoid'), \n",
    "\n",
    "parameters = { #Estos son datos a probar Creamos un diccionario y le ponemos lo que creemos que pruebe, en este caso linear, rbd, sgmoid\n",
    "    'kernel':('linear', 'rbf', 'sigmoid'), \n",
    "    'C':[0.0001,0.1, 0.5, 1, 5, 10, 100], \n",
    "    'degree': [1,2,3,4,5,6,7,8,9],\n",
    "    'coef0': [-10.,-1., 0., 0.1, 0.5, 1, 10, 100],\n",
    "    'gamma': ('scale', 'auto')\n",
    "    }\n",
    "\n",
    "svc = svm.SVC() #Instanciamos la clase del modelo que queremos utilizar\n",
    "\n",
    "clf = GridSearchCV(estimator=svc, param_grid=parameters, n_jobs=-1, cv=10)  #llamamos a grid search y le decimos que llame a ese algoritmo cv= significa q le pasamos un kfold especifico.\n",
    "clf.fit(iris.data, iris.target) # Hacemos un fit y le pasamos los datos.  IRis es un dataset que tiene los datos de las flores. X = iris.data, y = iris.tardet\n",
    "\n",
    "print(\"clf.best_stimator_\", clf.best_estimator_)\n",
    "print(\"clf.best_params_\", clf.best_params_)\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"clf.best_score\", clf.best_score_) #Me halla el mejor score\n",
    "\n",
    "#el mejor que me sale es el SVC con C=0.5, y con los demas datos que esta en el print\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Almost-Pro way\n",
    "\n",
    "La forma pro es la que hace esto mismo y va recogiendo los errores de entrenamiento, de validación y tiene la capacidad de parar el proceso cuando se requiera además de guardar el modelo en local una vez terminado si es mejor que el que había anteriormente y de cargar el modelo anterior y seguir reentrenando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # Para cargar una variable local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split \n",
    "# Set random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_test = np.arange(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline.  Pipeline a parte de probar gridsearch con un modelo sino con varios parametros que hemos definido.  El pipeline lo que va a decir es crearme una serie de pasos donde va a probar lo que pone despues de steps.  En el pipeline se cambiara el classficador a probar, es decir, los parametros de cada clasifcador (Classifier que es el algoritmo)\n",
    "\n",
    "# Le podemos poner cualquier clasificador. Irá cambiando según va probando pero necesita 1.\n",
    "pipe = Pipeline(steps=[('classifier', RandomForestClassifier())])\n",
    "\n",
    "#Hay que poner Classifier dos barras bajas y luego el nombre del parametro.  \n",
    "logistic_params = { \n",
    "    'classifier': [LogisticRegression()],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__C': np.logspace(0, 4, 10)\n",
    "    }\n",
    "\n",
    "random_forest_params = {\n",
    "    'classifier': [RandomForestClassifier()],\n",
    "    'classifier__n_estimators': [10, 100, 1000],\n",
    "    'classifier__max_features': [1, 2, 3]\n",
    "    }\n",
    "\n",
    "svm_params = {\n",
    "    'classifier': [svm.SVC()],\n",
    "    'classifier__kernel':('linear', 'rbf', 'sigmoid'), \n",
    "    'classifier__C':[0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], \n",
    "    'classifier__degree': to_test,\n",
    "    'classifier__coef0': [-10.,-1., 0., 0.1, 0.5, 1, 10, 100],\n",
    "    'classifier__gamma': ('scale', 'auto')\n",
    "    }\n",
    "\n",
    "#Para saber que parametros osn los mejores, se llama hypertuning\n",
    "\n",
    "# Create space of candidate learning algorithms and their hyperparameters. Tenemos que crear el espacio de busuqeda, y le damos los dicccionarios a buscar. POdriamos poner mas modelos si lo necesitamos\n",
    "search_space = [\n",
    "    logistic_params,\n",
    "    random_forest_params,\n",
    "    svm_params\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 10 folds for each of 3485 candidates, totalling 34850 fits\n",
      "\n",
      "############################\n",
      "\n",
      "best estimator: SVC(C=0.5, coef0=-10.0, degree=1, kernel='linear')\n",
      "\n",
      "############################\n",
      "\n",
      "clf.best_params_ {'classifier': SVC(C=0.5, coef0=-10.0, degree=1, kernel='linear'), 'classifier__C': 0.5, 'classifier__coef0': -10.0, 'classifier__degree': 1, 'classifier__gamma': 'scale', 'classifier__kernel': 'linear'}\n",
      "\n",
      "############################\n",
      "\n",
      "clf.best_score 0.9916666666666666\n"
     ]
    }
   ],
   "source": [
    "# PAra que nos de la info que nos ha tardado la ejecucion ?\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=1, random_state=1) \n",
    "# Create grid search \n",
    "clf = GridSearchCV(estimator=pipe, param_grid=search_space, cv=cv, verbose=10, n_jobs=-1) #estimator= pipe, se le pasa la variable de pipe (que ya esta creado). Si queremos que pruebe parametros diferentes para varios modelo le pasamos el pipe line que se va a ocupar de probar cada algoritmo con sus parametros.  Le pasamos param grid que es la lista de parametros, lo hacemos con cv que s cross validation.\n",
    "\n",
    "#VERBOSE = cuanto mas alto pongamos el numero de verbose, mas informacion me va a dar al hacer el print.  Las tareas que tiene que hacer es probar cada algoritmo con cada una de todas las opciones de parametros que hay es lo que ahace, y el verbose me pone cuanto esta tardando por tareas.   Esto esta bien para ver si nos hemos pasado con el numero de parametros que hemos puesto.  Me pone loq ue se tarda en x numero d tasks.  Si veo que tarda mucho para x tareas entonces alli puedo saber lo que puede tardar y yo mismo tomar la desicion.  Con esto mas o menos podemos saber lo que puede tardar si pusiera mas datos, hariamos una retgla de tres para saber.\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "# View best model\n",
    "separator = \"\\n############################\\n\"\n",
    "print(separator)\n",
    "print(\"best estimator:\", best_model.best_estimator_.get_params()['classifier']) #Que me muestre el mejor estimador con el mejor modelo\n",
    "print(separator)\n",
    "print(\"clf.best_params_\", clf.best_params_)\n",
    "print(separator)\n",
    "# Mean cross-validated score of the best_estimator\n",
    "print(\"clf.best_score\", clf.best_score_)\n",
    "#SAVE MODEL\n",
    "# save the model to disk\n",
    "filename = 'finished_model.sav' #Guardamos el modelo una vez que ya hemos entrenado el modelo\n",
    "pickle.dump(best_model, open(filename, 'wb')) #Lo guardamos en una variable\n",
    "\n",
    "#Esta probando cada modelo con todos los parametros que hemos puesto, por eso tarda varios segundos en ejecutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"finished_model.sav\", \"rb\")) #cargamos el modelo, pero si que el tenia en clases 2 lineas mas con el path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=10, random_state=1),\n",
       "             estimator=Pipeline(steps=[('classifier',\n",
       "                                        RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'classifier': [LogisticRegression()],\n",
       "                          'classifier__C': array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "       3.59381366...\n",
       "                          'classifier__n_estimators': [10, 100, 1000]},\n",
       "                         {'classifier': [SVC(C=0.5, coef0=-10.0, degree=1,\n",
       "                                             kernel='linear')],\n",
       "                          'classifier__C': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,\n",
       "                                            0.9],\n",
       "                          'classifier__coef0': [-10.0, -1.0, 0.0, 0.1, 0.5, 1,\n",
       "                                                10, 100],\n",
       "                          'classifier__degree': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                          'classifier__gamma': ('scale', 'auto'),\n",
       "                          'classifier__kernel': ('linear', 'rbf', 'sigmoid')}],\n",
       "             verbose=10)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Predict target vector\n",
    "best_model.score(X_test, y_test) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Another way - No pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading the Digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# To apply an classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "X = digits.images.reshape((n_samples, -1))\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0) #0.5 lo ponemos porque pq a medida que aumentamos el conjunto de test para probar algo, sabemos que a medida que si aumentamos el conjunto de test tambien se obtiene un buen acierto.  Si con un 50% de datos me sale que tengo un 99% de datos, es que mi modelo esta muy bien.  Siginificaria que nuestros datos para los modelos quiere decir que tiene simetria, y que se puede encontrar patrones relevantes.\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], \n",
    "                     'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "\n",
    "                      {'kernel': ['linear'], \n",
    "                    'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall'] #Son metricas.. El que estamos acostrumbrado es accuracy q es el tipico score que estabamos acostumbrados creado.  Lo podemos meter en eta lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "######################################\n########## SCORE precision ########### \n######################################\n# Tuning hyper-parameters for precision\n\nBest parameters set found on development set:\n\n{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n\nGrid scores on development set:\n\n0.983 (+/-0.015) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.956 (+/-0.027) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.985 (+/-0.014) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.981 (+/-0.020) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.985 (+/-0.014) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.981 (+/-0.019) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.985 (+/-0.014) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.981 (+/-0.019) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.976 (+/-0.002) for {'C': 1, 'kernel': 'linear'}\n0.976 (+/-0.002) for {'C': 10, 'kernel': 'linear'}\n0.976 (+/-0.002) for {'C': 100, 'kernel': 'linear'}\n0.976 (+/-0.002) for {'C': 1000, 'kernel': 'linear'}\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        89\n           1       0.97      1.00      0.98        90\n           2       0.99      0.98      0.98        92\n           3       1.00      0.99      0.99        93\n           4       1.00      1.00      1.00        76\n           5       0.99      0.98      0.99       108\n           6       0.99      1.00      0.99        89\n           7       0.99      1.00      0.99        78\n           8       1.00      0.98      0.99        92\n           9       0.99      0.99      0.99        92\n\n    accuracy                           0.99       899\n   macro avg       0.99      0.99      0.99       899\nweighted avg       0.99      0.99      0.99       899\n\n\n######################################\n########## SCORE recall ########### \n######################################\n# Tuning hyper-parameters for recall\n\nBest parameters set found on development set:\n\n{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n\nGrid scores on development set:\n\n0.982 (+/-0.016) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n0.953 (+/-0.027) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.984 (+/-0.016) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n0.981 (+/-0.020) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.984 (+/-0.016) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n0.980 (+/-0.019) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.984 (+/-0.016) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n0.980 (+/-0.019) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n0.974 (+/-0.005) for {'C': 1, 'kernel': 'linear'}\n0.974 (+/-0.005) for {'C': 10, 'kernel': 'linear'}\n0.974 (+/-0.005) for {'C': 100, 'kernel': 'linear'}\n0.974 (+/-0.005) for {'C': 1000, 'kernel': 'linear'}\nDetailed classification report:\n\nThe model is trained on the full development set.\nThe scores are computed on the full evaluation set.\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        89\n           1       0.97      1.00      0.98        90\n           2       0.99      0.98      0.98        92\n           3       1.00      0.99      0.99        93\n           4       1.00      1.00      1.00        76\n           5       0.99      0.98      0.99       108\n           6       0.99      1.00      0.99        89\n           7       0.99      1.00      0.99        78\n           8       1.00      0.98      0.99        92\n           9       0.99      0.99      0.99        92\n\n    accuracy                           0.99       899\n   macro avg       0.99      0.99      0.99       899\nweighted avg       0.99      0.99      0.99       899\n\n\n"
    }
   ],
   "source": [
    "bar = \"######################################\"\n",
    "for score in scores:\n",
    "    print(bar + \"\\n########## SCORE \" + score + \" ########### \\n\" + bar)\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(estimator=SVC(), param_grid=tuned_parameters, scoring='%f_macro'%score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score'] #cv_results es que se está haciendo con un cross_validation especifico. y a cada uno de ellos le saco la media del test score\n",
    "    stds = clf.cv_results_['std_test_score'] #Esto es la media del no se que\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred)) #Le pasamos lo que debe dar y la prediccion, nos da el reporte.  ME da un f1 score de 0.99, casi el 100%.\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}